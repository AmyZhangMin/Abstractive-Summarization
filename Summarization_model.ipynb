{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embedding[0])\n",
    "#Pre-trained GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "        if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "            return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('vec_summaries', 'rb') as fp:\n",
    "    vec_summaries = pickle.load(fp)\n",
    "\n",
    "with open ('vec_texts', 'rb') as fp:\n",
    "    vec_texts = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('vocab_limit', 'rb') as fp:\n",
    "    vocab_limit = pickle.load(fp)\n",
    "\n",
    "with open ('embd_limit', 'rb') as fp:\n",
    "    embd_limit = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dataset with summary length beyond 7: 16.146% \n",
      "Percentage of dataset with text length less that window size: 2.258% \n",
      "Percentage of dataset with text length more than 80: 40.412% \n"
     ]
    }
   ],
   "source": [
    "#DIAGNOSIS\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 7\n",
    "\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with summary length beyond \"+str(LEN)+\": \"+str((count/len(vec_summaries))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "D = 10 \n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)<window_size+1:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length less that window size: \"+str((count/len(vec_texts))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 80\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length more than \"+str(LEN)+\": \"+str((count/len(vec_texts))*100)+\"% \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SUMMARY_LEN = 7\n",
    "MAX_TEXT_LEN = 80\n",
    "\n",
    "#D is a major hyperparameters. Windows size for local attention will be 2*D+1\n",
    "D = 10\n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "#REMOVE DATA WHOSE SUMMARIES ARE TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS SMALLED THAN WINDOW SIZE\n",
    "\n",
    "vec_summaries_reduced = []\n",
    "vec_texts_reduced = []\n",
    "\n",
    "i = 0\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1<=MAX_SUMMARY_LEN and len(vec_texts[i])>=window_size and len(vec_texts[i])<=MAX_TEXT_LEN:\n",
    "        vec_summaries_reduced.append(summary)\n",
    "        vec_texts_reduced.append(vec_texts[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_len = int((.7)*len(vec_summaries_reduced))\n",
    "\n",
    "train_texts = vec_texts_reduced[0:train_len]\n",
    "train_summaries = vec_summaries_reduced[0:train_len]\n",
    "\n",
    "val_len = int((.15)*len(vec_summaries_reduced))\n",
    "\n",
    "val_texts = vec_texts_reduced[train_len:train_len+val_len]\n",
    "val_summaries = vec_summaries_reduced[train_len:train_len+val_len]\n",
    "\n",
    "test_texts = vec_texts_reduced[train_len+val_len:len(vec_summaries_reduced)]\n",
    "test_summaries = vec_summaries_reduced[train_len+val_len:len(vec_summaries_reduced)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18293\n"
     ]
    }
   ],
   "source": [
    "print train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_out(output_text):\n",
    "    output_len = len(output_text)\n",
    "    transformed_output = np.zeros([output_len],dtype=np.int32)\n",
    "    for i in xrange(0,output_len):\n",
    "        transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n",
    "    return transformed_output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some MORE hyperparameters and other stuffs\n",
    "\n",
    "hidden_size = 250\n",
    "learning_rate = 0.003\n",
    "K = 5\n",
    "vocab_len = len(vocab_limit)\n",
    "training_iters = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#placeholders\n",
    "tf_text = tf.placeholder(tf.float32, [None,word_vec_dim])\n",
    "tf_seq_len = tf.placeholder(tf.int32)\n",
    "tf_summary = tf.placeholder(tf.int32,[None])\n",
    "tf_output_len = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_encoder(inp,hidden,Wxh,Whh,Wattention,B,seq_len,inp_dim):\n",
    "\n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=0\n",
    "    j=K\n",
    "    \n",
    "    def fn(hidden_residuals):\n",
    "        \n",
    "        return hidden_residuals\n",
    "    \n",
    "    def cond(i,j,hidden,hidden_forward,hidden_residuals):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,j,hidden,hidden_forward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        hidden_next = tf.nn.elu(tf.matmul(x,Wxh) + tf.matmul(hidden,Whh) + B + RRA)\n",
    "        \n",
    "        hidden = hidden_next\n",
    "        \n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "\n",
    "        hidden_forward = hidden_forward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i+1,j+1,hidden,hidden_forward,hidden_residuals\n",
    "    \n",
    "    _,_,_,hidden_forward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,hidden_forward,hidden_residuals])\n",
    "    \n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_forward.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_encoder(inp,hidden,Wxh,Whh,Wattention,B,seq_len,inp_dim):\n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=seq_len-1\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,hidden_backward,hidden_residuals):\n",
    "        return i > -1\n",
    "    \n",
    "    def body(i,j,hidden,hidden_backward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        hidden_next = tf.nn.elu(tf.matmul(x,Wxh) + tf.matmul(hidden,Whh) + B + RRA)\n",
    "        \n",
    "        hidden = hidden_next\n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "        \n",
    "        hidden_backward = hidden_backward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i-1,j+1,hidden,hidden_backward,hidden_residuals\n",
    "    \n",
    "    _,_,_,hidden_backward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,hidden_backward,hidden_residuals])\n",
    "\n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_backward.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(inp,hidden,Wxh,Whh,B,RRA):\n",
    "    hidden_next = tf.nn.elu(tf.matmul(inp,Wxh) + tf.matmul(hidden,Whh) + B + RRA)\n",
    "    \n",
    "    return hidden_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(hs,ht,Wa,seq_len):\n",
    "    return tf.reshape(tf.matmul(tf.matmul(hs,Wa),tf.transpose(ht)),[seq_len])\n",
    "\n",
    "def align(hs,ht,Wp,Vp,Wa,tf_seq_len):\n",
    "   \n",
    "    pd = tf.TensorArray(size=(2*D+1),dtype=tf.float32)\n",
    "    \n",
    "    sequence_length = tf_seq_len-tf.constant((2*D+1),dtype=tf.int32)\n",
    "    sequence_length = tf.cast(sequence_length,dtype=tf.float32)\n",
    "    \n",
    "    pt_float = tf.multiply(sequence_length,tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(ht,Wp)),Vp)))\n",
    "    pt_float = tf.add(pt_float,tf.cast(D,tf.float32))\n",
    "    \n",
    "    pt_float = tf.reshape(pt_float,[])\n",
    "    \n",
    "    pt = tf.cast(pt_float,tf.int32)\n",
    "    \n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    i = 0\n",
    "    pos = pt - D\n",
    "    \n",
    "    def cond(i,pos,pd):\n",
    "        \n",
    "        return i < (2*D+1)\n",
    "                      \n",
    "    def body(i,pos,pd):\n",
    "        \n",
    "        comp_1 = tf.cast(tf.square(tf.cast(pos,tf.float32)-pt_float),tf.float32)\n",
    "        comp_2 = tf.cast(2*tf.square(sigma),tf.float32)\n",
    "            \n",
    "        pd = pd.write(i,tf.exp(-(comp_1/comp_2)))\n",
    "            \n",
    "        return i+1,pos+1,pd\n",
    "                      \n",
    "    i,pos,pd = tf.while_loop(cond,body,[i,pos,pd])\n",
    "    \n",
    "    local_hs = hs[(pt-D):(pt+D+1)]\n",
    "    \n",
    "    normalized_scores = tf.nn.softmax(score(local_hs,ht,Wa,2*D+1))\n",
    "    \n",
    "    pd=pd.stack()\n",
    "    \n",
    "    G = tf.multiply(normalized_scores,pd)\n",
    "    G = tf.reshape(G,[2*D+1,1])\n",
    "    \n",
    "    return G,pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(tf_text,tf_seq_len,tf_output_len):\n",
    "    \n",
    "    #PARAMETERS\n",
    "    \n",
    "    #1.1 FORWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    Wxh_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    Whh_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    B_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_f = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "                               \n",
    "    #1.2 BACKWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    Wxh_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    Whh_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    B_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_b = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    #2 ATTENTION PARAMETERS\n",
    "    \n",
    "    Wp = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,50],stddev=0.01))\n",
    "    Vp = tf.Variable(tf.truncated_normal(shape=[50,1],stddev=0.01))\n",
    "    Wa = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    Wc = tf.Variable(tf.truncated_normal(shape=[4*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    \n",
    "    #3 DECODER PARAMETERS\n",
    "    \n",
    "    Ws = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,vocab_len],stddev=0.01))\n",
    "    \n",
    "    Wxh_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    Whh_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    B_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals_d = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals_d = hidden_residuals_d.unstack(tf.zeros([K,2*hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    Wattention_d = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    output = tf.TensorArray(size=tf_output_len,dtype=tf.float32)\n",
    "                               \n",
    "    #BI-DIRECTIONAL ENCODER\n",
    "                               \n",
    "    hidden_forward = forward_encoder(tf_text,\n",
    "                                     initial_hidden_f,\n",
    "                                     Wxh_f,Whh_f,Wattention_f,B_f,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    hidden_backward = backward_encoder(tf_text,\n",
    "                                       initial_hidden_b,\n",
    "                                       Wxh_b,Whh_b,Wattention_b,B_b,\n",
    "                                       tf_seq_len,\n",
    "                                       word_vec_dim)\n",
    "    \n",
    "    encoded_hidden = tf.concat([hidden_forward,hidden_backward],1)\n",
    "    \n",
    "    #ATTENTION MECHANISM AND DECODER\n",
    "    \n",
    "    decoded_hidden = encoded_hidden[0]\n",
    "    decoded_hidden = tf.reshape(decoded_hidden,[1,2*hidden_size])\n",
    "    Wattention_d_normalized = tf.nn.softmax(Wattention_d)\n",
    "                               \n",
    "    i=0\n",
    "    j=K\n",
    "    \n",
    "    def attention_decoder_cond(i,j,decoded_hidden,hidden_residuals_d,output):\n",
    "        return i < tf_output_len\n",
    "    \n",
    "    def attention_decoder_body(i,j,decoded_hidden,hidden_residuals_d,output):\n",
    "        \n",
    "        #LOCAL ATTENTION\n",
    "        \n",
    "        G,pt = align(encoded_hidden,decoded_hidden,Wp,Vp,Wa,tf_seq_len)\n",
    "        local_encoded_hidden = encoded_hidden[pt-D:pt+D+1]\n",
    "        weighted_encoded_hidden = tf.multiply(local_encoded_hidden,G)\n",
    "        context_vector = tf.reduce_sum(weighted_encoded_hidden,0)\n",
    "        context_vector = tf.reshape(context_vector,[1,2*hidden_size])\n",
    "        \n",
    "        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,decoded_hidden],1),Wc))\n",
    "        \n",
    "        #DECODER\n",
    "        \n",
    "        y = tf.matmul(attended_hidden,Ws)\n",
    "        \n",
    "        output = output.write(i,tf.reshape(y,[vocab_len]))\n",
    "        \n",
    "        y = tf.nn.softmax(y)\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "        RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "        \n",
    "        decoded_hidden_next = decoder(y,attended_hidden,Wxh_d,Whh_d,B_d,RRA)\n",
    "        \n",
    "        decoded_hidden = decoded_hidden_next\n",
    "        \n",
    "        hidden_residuals_d = tf.cond(tf.equal(j,tf_output_len-1+K),\n",
    "                                   lambda: hidden_residuals_d,\n",
    "                                   lambda: hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size])))\n",
    "        \n",
    "        return i+1,j+1,decoded_hidden,hidden_residuals_d,output\n",
    "    \n",
    "    i,j,decoded_hidden,hidden_residuals_d,output = tf.while_loop(attention_decoder_cond,\n",
    "                                            attention_decoder_body,\n",
    "                                            [i,j,decoded_hidden,hidden_residuals_d,output])\n",
    "    hidden_residuals_d.close().mark_used()\n",
    "    \n",
    "    output = output.stack()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tf_text,tf_seq_len,tf_output_len)\n",
    "\n",
    "#OPTIMIZER\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=tf_summary))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#PREDICTION\n",
    "\n",
    "pred = tf.TensorArray(size=tf_output_len,dtype=tf.int32)\n",
    "\n",
    "i=0\n",
    "\n",
    "def cond_pred(i,pred):\n",
    "    return i<tf_output_len\n",
    "def body_pred(i,pred):\n",
    "    pred = pred.write(i,tf.cast(tf.argmax(output[i]),tf.int32))\n",
    "    return i+1,pred\n",
    "\n",
    "i,pred = tf.while_loop(cond_pred,body_pred,[i,pred]) \n",
    "\n",
    "prediction = pred.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 0\n",
      "Training input sequence length: 51\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i have bought several of the vitality canned dog food products and have found them all to be of good quality. the product looks more like a stew than a processed meat and it smells better. my labrador is finicky and she appreciates this product better than most.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "competition 1960 cecco cecco\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good quality dog food\n",
      "\n",
      "loss=10.4954\n",
      "\n",
      "Iteration: 1\n",
      "Training input sequence length: 37\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product arrived labeled as jumbo salted peanuts ... the peanuts were actually small sized unsalted. not sure if this was an error or if the vendor intended to represent the product as `` jumbo ''.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "its food 15.6\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "not as advertised\n",
      "\n",
      "loss=10.245\n",
      "\n",
      "Iteration: 2\n",
      "Training input sequence length: 46\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered( which was good) and made some cherry soda. the flavor is very medicinal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "full-bodied dog\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "cough medicine\n",
      "\n",
      "loss=10.5442\n",
      "\n",
      "Iteration: 3\n",
      "Training input sequence length: 32\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "great taffy at a great price. there was a wide assortment of yummy taffy. delivery was very quick. if your a taffy lover, this is a deal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not advertised\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great taffy\n",
      "\n",
      "loss=10.4141\n",
      "\n",
      "Iteration: 4\n",
      "Training input sequence length: 30\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this taffy is so good. it is very soft and chewy. the flavors are amazing. i would definitely recommend you buying it. very satisfying!!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not advertised advertised advertised\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wonderful, tasty taffy\n",
      "\n",
      "loss=10.05\n",
      "\n",
      "Iteration: 5\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "right now i 'm mostly just sprouting this so my cats can eat the grass. they love it. i rotate it around with wheatgrass and rye too\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "yay barley\n",
      "\n",
      "loss=10.4446\n",
      "\n",
      "Iteration: 6\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "this is a very healthy dog food. good for their digestion. also good for small puppies. my dog eats her required amount at every feeding.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "cough food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "healthy dog food\n",
      "\n",
      "loss=8.06794\n",
      "\n",
      "Iteration: 7\n",
      "Training input sequence length: 24\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "the strawberry twizzlers are my guilty pleasure- yummy. six pounds will be around for a while with my son and i.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "strawberry twizzlers- yummy\n",
      "\n",
      "loss=10.3926\n",
      "\n",
      "Iteration: 8\n",
      "Training input sequence length: 45\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i love eating them and they are good for watching tv and looking at movies! it is not too sweet. i like to transfer them to a zip lock baggie so they stay fresh so i can take my time eating them.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "poor taste\n",
      "\n",
      "loss=10.4998\n",
      "\n",
      "Iteration: 9\n",
      "Training input sequence length: 28\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i am very satisfied with my unk purchase. i shared these with others and we have all enjoyed them. i will definitely be ordering more.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "love it!\n",
      "\n",
      "loss=10.5893\n",
      "\n",
      "Iteration: 10\n",
      "Training input sequence length: 31\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "candy was delivered very fast and was purchased at a reasonable price. i was home bound and unable to get to a store so this was perfect for me.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "home delivered unk\n",
      "\n",
      "loss=10.6324\n",
      "\n",
      "Iteration: 11\n",
      "Training input sequence length: 52\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "my husband is a twizzlers addict. we 've bought these many times from amazon because we 're government employees living overseas and ca n't get them in the country we are assigned to. they 've always been fresh and tasty, packed well and arrive in a timely manner.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "always fresh\n",
      "\n",
      "loss=10.8797\n",
      "\n",
      "Iteration: 12\n",
      "Training input sequence length: 68\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i bought these for my husband who is currently overseas. he loves these, and apparently his staff likes them unk< br/> there are generous amounts of twizzlers in each 16-ounce bag, and this was well worth the price.< a unk '' http: unk ''> twizzlers, strawberry, 16-ounce bags( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "loss=8.25381\n",
      "\n",
      "Iteration: 13\n",
      "Training input sequence length: 31\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i can remember buying this candy as a kid and the quality has n't dropped in all these years. still a superb product you wo n't be disappointed with.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "delicious product!\n",
      "\n",
      "loss=9.78395\n",
      "\n",
      "Iteration: 14\n",
      "Training input sequence length: 21\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i love this candy. after weight watchers i had to cut back but still have a craving for it.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "loss=5.16462\n",
      "\n",
      "Iteration: 15\n",
      "Training input sequence length: 72\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "i have lived out of the us for over 7 yrs now, and i so miss my twizzlers!! when i go back to visit or someone visits me, i always stock up. all i can say is yum!< br/> sell these in mexico and you will have a faithful buyer, more often than i 'm able to buy them right now.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful food food food food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "please sell these in mexico!!\n",
      "\n",
      "loss=10.1993\n",
      "\n",
      "Iteration: 16\n",
      "Training input sequence length: 36\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product received is as unk< br/>< br/>< a unk '' http: unk ''> twizzlers, strawberry, 16-ounce bags( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "wonderful taffy taffy\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers- strawberry\n",
      "\n",
      "loss=4.43463\n",
      "\n",
      "Iteration: 17\n",
      "Training input sequence length: 43\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "i was so glad amazon carried these batteries. i have a hard time finding them elsewhere because they are such a unique size. i need them for my garage door unk< br/> great deal for the price.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers taffy taffy taffy taffy\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great bargain for the price\n",
      "\n",
      "loss=11.2824\n",
      "\n",
      "Iteration: 18\n",
      "Training input sequence length: 26\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "this offer is a great price and a great taste, thanks amazon for selling this unk< br/>< br/> unk\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers as as as as\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "this is my taste ...\n",
      "\n",
      "loss=10.4893\n",
      "\n",
      "Iteration: 19\n",
      "Training input sequence length: 60\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "for those of us with celiac disease this product is a lifesaver and what could be better than getting it at almost half the price of the grocery or health food store! i love mccann 's instant oatmeal- all flavors!!!< br/>< br/> thanks,< br/> abby\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "love gluten free oatmeal!!!\n",
      "\n",
      "loss=7.05802\n",
      "\n",
      "Iteration: 20\n",
      "Training input sequence length: 59\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "what else do you need to know? oatmeal, instant( make it with a half cup of low-fat milk and add raisins; nuke for 90 seconds). more expensive than kroger store brand oatmeal and maybe a little tastier or better texture or something. it 's still just oatmeal. mmm, convenient!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "it 's oatmeal\n",
      "\n",
      "loss=11.5547\n",
      "\n",
      "Iteration: 21\n",
      "Training input sequence length: 79\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ordered this for my wife as it was unk by our daughter. she has this almost every morning and likes all flavors. she 's happy, i 'm happy!!!< br/>< a unk '' http: unk ''> mccann 's instant irish oatmeal, variety pack of regular, apples& cinnamon, and maple& brown sugar, 10-count boxes( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wife 's favorite breakfast\n",
      "\n",
      "loss=14.5898\n",
      "\n",
      "Iteration: 22\n",
      "Training input sequence length: 38\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i have mccann 's oatmeal every morning and by ordering it from amazon i am able to save almost$ 3.00 per unk< br/> it is a great product. tastes great and very healthy\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "unk\n",
      "\n",
      "loss=7.63945\n",
      "\n",
      "Iteration: 23\n",
      "Training input sequence length: 41\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "mccann 's oatmeal is a good quality choice. our favorite is the apples and cinnamon, but we find that none of these are overly sugary. for a good hot breakfast in 2 minutes, this is excellent.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good hot breakfast\n",
      "\n",
      "loss=11.1365\n",
      "\n",
      "Iteration: 24\n",
      "Training input sequence length: 55\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "we really like the mccann 's steel cut oats but find we do n't cook it up too unk< br/> this tastes much better to me than the grocery store brands and is just as unk< br/> anything that keeps me eating oatmeal regularly is a good thing.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great taste and convenience\n",
      "\n",
      "loss=8.83746\n",
      "\n",
      "Iteration: 25\n",
      "Training input sequence length: 46\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "this seems a little more wholesome than some of the supermarket brands, but it is somewhat mushy and does n't have quite as much flavor either. it did n't pass muster with my kids, so i probably wo n't buy it again.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "hearty oatmeal\n",
      "\n",
      "loss=12.347\n",
      "\n",
      "Iteration: 26\n",
      "Training input sequence length: 52\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "good oatmeal. i like the apple cinnamon the best. though i would n't follow the directions on the package since it always comes out too soupy for my taste. that could just be me since i like my oatmeal really thick to add some milk on top of.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good\n",
      "\n",
      "loss=3.61977\n",
      "\n",
      "Iteration: 27\n",
      "Training input sequence length: 25\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "the flavors are good. however, i do not see any unk between this and unk oats brand- they are both mushy.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "mushy\n",
      "\n",
      "loss=16.738\n",
      "\n",
      "Iteration: 28\n",
      "Training input sequence length: 41\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "this is the same stuff you can buy at the big box stores. there is nothing healthy about it. it is just carbs and sugars. save your money and get something that at least has some taste.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers taste\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "same stuff\n",
      "\n",
      "loss=15.9183\n",
      "\n",
      "Iteration: 29\n",
      "Training input sequence length: 25\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this oatmeal is not good. its mushy, soft, i do n't like it. quaker oats is the way to go.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "good taste taste taste\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "do n't like it\n",
      "\n",
      "loss=12.6183\n",
      "\n",
      "Iteration: 30\n",
      "Training input sequence length: 37\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "we 're used to spicy foods down here in south texas and these are not at all spicy. doubt very much habanero is used at all. could take it up a notch or two.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "good taste taste\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "not ass kickin\n",
      "\n",
      "loss=11.6902\n",
      "\n",
      "Iteration: 31\n",
      "Training input sequence length: 80\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "i roast at home with a unk popcorn popper( but i do it outside, of course). these beans( coffee bean direct green mexican altura) seem to be well-suited for this method. the first and second cracks are distinct, and i 've roasted the beans from medium to slightly dark with great results every time. the aroma is strong and persistent. the taste is smooth, velvety, yet"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from __future__ import print_function\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    display_step = 1\n",
    "    \n",
    "    while step < training_iters:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "           \n",
    "        for i in xrange(0,train_len):\n",
    "            \n",
    "            train_out = transform_out(train_summaries[i][0:len(train_summaries[i])-1])\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nIteration: \"+str(i))\n",
    "                print(\"Training input sequence length: \"+str(len(train_texts[i])))\n",
    "                print(\"Training target outputs sequence length: \"+str(len(train_out)))\n",
    "            \n",
    "                print(\"\\nTEXT:\")\n",
    "                flag = 0\n",
    "                for vec in train_texts[i]:\n",
    "                    if vec2word(vec) in string.punctuation or flag==0:\n",
    "                        print(str(vec2word(vec)),end='')\n",
    "                    else:\n",
    "                        print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,pred = sess.run([optimizer,cost,prediction],feed_dict={tf_text: train_texts[i], \n",
    "                                                    tf_seq_len: len(train_texts[i]), \n",
    "                                                    tf_summary: train_out,\n",
    "                                                    tf_output_len: len(train_out)})\n",
    "            \n",
    "         \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for index in pred:\n",
    "                    #if int(index)!=vocab_limit.index('eos'):\n",
    "                    if vocab_limit[int(index)] in string.punctuation or flag==0:\n",
    "                        print(str(vocab_limit[int(index)]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab_limit[int(index)]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for vec in train_summaries[i]:\n",
    "                    if vec2word(vec)!='eos':\n",
    "                        if vec2word(vec) in string.punctuation or flag==0:\n",
    "                            print(str(vec2word(vec)),end='')\n",
    "                        else:\n",
    "                            print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "            \n",
    "                #print(hs)\n",
    "            \n",
    "                print(\"loss=\"+str(loss))\n",
    "            \n",
    "            #print(h)\n",
    "            #print(out)\n",
    "            #print(ht_s)\n",
    "            \n",
    "        step=step+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
